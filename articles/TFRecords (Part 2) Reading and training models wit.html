<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>TFRecords (Part 2): Reading and training models with Tfrecords.</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-interactiveBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}

.back-to-home {
    position: absolute;
    top: 20px;
    left: 20px;
    z-index: 1;
}

.back-to-home a {
    text-decoration: none;
    color: #fff;
    font-weight: bold;
    background-color: rgba(0, 0, 0, 0.7);
    padding: 6px 10px;
    border-radius: 4px;
}
	
</style></head><body><article id="d3d55dde-3c4a-4484-a2dc-ead25d989033" class="page sans">
	<header>
		<div class="back-to-home">
			<a href="/">Back to Home</a>
		</div>
		
		<img class="page-cover-image" src="https://www.notion.so/images/page-cover/rijksmuseum_mignons_1660.jpg" style="object-position:center 50%"/><div class="page-header-icon page-header-icon-with-cover"><span class="icon">üòÖ</span></div><h1 class="page-title"><strong>TFRecords (Part 2): Reading and training models with Tfrecords.</strong></h1><p class="page-description"></p><table class="properties"><tbody></tbody></table></header><div class="page-body"><p id="e5805d38-c253-4042-91a4-de8377de8a2c" class="">In the¬†<a href="https://medium.com/@sodipepaul/tfrecords-part-1-converting-a-dataset-into-tfrecord-files-7c7c2bdf0e4b">first part</a>, I showed how to convert a dataset of medical images and its target value into tfrecords. Here, I will be showing how to read tfrecords and also how to train an ML model in tensorflow using tfrecords. For this, I will be used the tfrecords I created from the first part.</p><h2 id="c7cc61cc-4c0e-4a59-9f85-7f94c58c07fb" class=""><strong><strong>Reading Tfrecords</strong></strong></h2><p id="92175027-75fc-4e75-81ba-55a08d0a9436" class="">Tfrecords store data in binary format for fast and easy access. To read an example from a tfrecord file, we first need to create a function to parse the example from the file and then use¬†<code>tf.data.TFRecordDataset</code>¬†object and the parsing function to read the file. This is a very easy and straightforward process.</p><p id="46992803-19cb-49ba-9f2b-fba138fac760" class="">In the first part, we put two features (image and target) from our dataset in the tfrecord files. First, we create a¬†<code>feature_description</code>¬†dictionary to read each feature. The keys in the dictionary must be the same as the keys used to store the features in the tfrecord files.¬†<code>tf.io.FixedLenFeature</code>¬†reads the features and stores them in the data type given. Here,¬†<code>tf.string</code>¬†is used for the¬†<strong>image</strong>¬†feature since it is a tensor and¬†<code>tf.int64</code>¬†is used for the target value since it‚Äôs an integer.</p><pre id="87f37d75-0641-4f43-941e-5e9d00b728e0" class="code code-wrap"><code>def parse_tfrecord_fn(example):
    feature_description = {
        &quot;image&quot;: tf.io.FixedLenFeature([], tf.string),
        &quot;target&quot;: tf.io.FixedLenFeature([], tf.int64)
    }
    example = tf.io.parse_single_example(example, feature_description)
    example[&quot;image&quot;] = tf.io.decode_png(example[&quot;image&quot;])
    return example</code></pre><p id="950743fe-066f-4d97-ba1e-477604b33004" class="">The¬†<code>parse_tfrecord_fn</code>¬†reads each example and maps the features to the corresponding data type. The¬†<code>tf.io.decode_png</code>¬†function does the opposite of what the¬†<code>tf.io.encode_png</code>¬†function introduced in Part 1 does. It converts the bytelist back to an image tensor using png compression.</p><p id="507dc2e3-f21c-47ec-8b8f-476f3d6603b1" class="">After creating the parsing function, it‚Äôs fairly easy to read the files. First, use¬†<code>tf.data.TFRecordDataset</code>¬†to read the tfrecord file from the path and call the map method using the¬†<code>parse_tfrecord_fn</code>¬†as the argument.</p><pre id="8391893a-92f8-43f3-942c-0b41a98f08b9" class="code code-wrap"><code>raw_dataset = tf.data.TFRecordDataset(&quot;.../tfrecords/tfrecord_0-1000.tfrec&quot;)
parsed_dataset = raw_dataset.map(parse_tfrecord_fn)</code></pre><p id="85621b96-97cf-437c-8305-7e1e1a648f6c" class="">Now that the dataset is parsed, to read an example from it, we use the take method, specifying the number of examples we want to read as the argument. This is similar to how we use¬†<code>df.head()</code>¬†in pandas.</p><pre id="764378d3-c185-4602-a37a-ac537f15f5ef" class="code code-wrap"><code>for example in parsed_dataset.take(5):
    for key in example.keys():
        print(f&quot;{key}: {type(key)}&quot;)

    print(f&quot;Image shape: {example[&#x27;image&#x27;].shape}&quot;)
    plt.figure(figsize=(7, 7))
    plt.imshow(example[&quot;image&quot;].numpy())
    plt.show()</code></pre><p id="2050abb4-4122-45a2-8e80-a3e2d5b1e199" class=""><code>parsed_dataset.take(5)</code>¬†takes five examples from the dataset. You can access the files using a for loop or by passing it into a list using¬†<code>list(parsed_dataset.take(5))</code>Either way, the examples are stored as a dictionary and the features and values are a key-value pair.</p><hr id="68079f1f-ffbd-4852-bdd9-0b02449b56dd"/><h1 id="90241465-4b90-4c3d-90bc-462293e0d398" class="">Training a tensorflow model using tfrecords</h1><p id="b056d7ec-93fd-4845-a15a-81e4db085679" class="">Training a tensorflow/Keras deep learning model using tfrecords is very easy. First, define the model.</p><p id="a4634905-6c20-492d-9e88-d8523ac0b8c9" class="">Since the dataset consists of images and their target value, I have chosen the vgg-16 model from¬†<code>tf.keras.applications</code>¬†and I will be applying transfer learning, so the model weights are set to¬†<code>imagenet</code>¬†and trainability of the¬†<code>vgg16</code>¬†is set to false. Here‚Äôs how it‚Äôs done using the Keras functional API.</p><pre id="634cfffd-cdc6-4b7b-9be8-327348941179" class="code code-wrap"><code>num_classes = 1
input_image = tf.keras.Input(shape=(224, 224, 3), name=&#x27;image&#x27;)

# Load the VGG16 model
vgg16 = tf.keras.applications.VGG16(weights=&#x27;imagenet&#x27;,
                                    include_top=False,
                                    input_shape=(224, 224, 3))

vgg16.trainable = False

x = vgg16(input_image)

y = tf.keras.layers.Flatten()(x)

z = tf.keras.layers.Dense(128, activation=&#x27;relu&#x27;)(y)

output = tf.keras.layers.Dense(num_classes, activation=&#x27;sigmoid&#x27;)(z)

# Create the model
model = tf.keras.Model(inputs=[input_image], outputs=output)

model.summary()</code></pre><p id="71e61348-ac5f-45ac-8c17-b79a31bb846f" class="">Line 2 of the code above defines the Input of the model. Here, we are using a tensor of shape¬†<code>(224, 224, 3)</code>¬†which is the size of the image and assigning the input the name ‚Äú<strong>image</strong>‚Äù. The name would help tensorflow know what feature from the tfrecord file to use as input. This is particularly helpful when you have more than one feature.</p><p id="557c3780-8aa3-49f2-b39a-44a3482ba1aa" class="">Next, we define a parsing function that will convert each example in a way that tensorflow will understand when trying to fit the model.</p><pre id="8661dadd-d714-4e71-bd30-35a03243b486" class="code code-wrap"><code>def parse_example(example):
    feature_description = {
        &quot;image&quot;: tf.io.FixedLenFeature([], tf.string),
        &quot;target&quot;: tf.io.FixedLenFeature([], tf.int64)
    }
    example = tf.io.parse_single_example(example, feature_description)
    example[&quot;image&quot;] = tf.io.decode_png(example[&quot;image&quot;])
    X = dict()
    X[&#x27;image&#x27;] = tf.image.grayscale_to_rgb(example[&#x27;image&#x27;])
    y = example[&#x27;target&#x27;]
    return X, y</code></pre><p id="5cda70e9-ea4d-4fdb-bc37-f2fa10324714" class="">Here, the function returns a tuple¬†<code>(X, y)</code>¬†where¬†<code>X</code>¬†is the feature and¬†<code>y</code>¬†the target. The first 6 lines of the function are the same as the one we used earlier. The main difference is the definition of a dictionary assigned to¬†<code>X</code>¬†. The next line takes the image tensor from¬†<code>example[&#x27;image&#x27;]</code>¬†and converts it from shape¬†<code>(224, 224, 1)</code>¬†to¬†<code>(224, 224, 3)</code>¬†.( You can read the documentation of how¬†<a href="https://www.tensorflow.org/api_docs/python/tf/image/grayscale_to_rgb"><code>tf.image.grayscale_to_rgb</code></a>¬†works for more details on that). The converted tensor is assigned to¬†<code>X[&#x27;image&#x27;]</code>¬†.¬†<code>X</code>¬†is the feature dictionary we want to return along with the target and¬†<code><strong>image</strong></code>¬†is used as the key here to match the name of the Input defined in the model above.</p><p id="38406e0e-540b-48fa-b99b-a989cf17d559" class="">Now, all we have to do is read the files. Since we created more than one tfrecord file. If we want to train the model on all of the data available, we have to read all the files. Thankfully,¬†<code>tf.data.TFRecordDataset</code>¬†accepts both a single file path and a list of file paths as input arguments.</p><pre id="394b853d-c6d4-433b-8a1a-8ca029113a64" class="code code-wrap"><code>tf_files = glob.glob(&#x27;/kaggle/input/notebook9fa8840645/tfrecords/*.tfrec&#x27;)
dataset = tf.data.TFRecordDataset(tf_files)
dataset = dataset.map(parse_example)</code></pre><p id="a7cb7f44-3722-4411-83ae-1103552528f6" class="">This time around, when calling the map method, we use the use parsing function¬†<code>parse_example</code>¬†as input.</p><p id="5c12a97d-fb61-43d3-a109-a111bd6d96a2" class="">Now, to train the model using the dataset, we call the¬†<code>batch</code>¬†method on the dataset and pass the output as the argument to¬†<code>model.fit</code></p><pre id="1c721de3-e339-452c-bf7d-21fe556854bb" class="code code-wrap"><code>dataset = dataset.shuffle(buffer_size=1024).batch(32)

model.compile(optimizer=&#x27;rmsprop&#x27;, loss=&#x27;binary_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])

# Train the model
model.fit(dataset, epochs=10, validation_data=val_dataset)</code></pre><p id="f31af537-cf94-4c40-a1c6-29c7ef45ba13" class="">Here, I called the¬†<code>shuffle</code>¬†method first to randomly select 1024 examples and then using the¬†<code>batch</code>¬†method to pick 32 of those.</p><hr id="64660b99-3829-4c6a-9506-6879183377f1"/><h1 id="99a36e08-7364-4f9a-b2de-1774446d3748" class="">How about a Train/Val/Test Split?</h1><p id="398b6973-9d2e-49e5-9d6c-759be0351d55" class=""><code>tf.data.TFRecordDataset</code>¬†does not provide a straightforward way to perform a train-test split. But it‚Äôs pretty much straightforward if you know the number of examples in the dataset. Tensorflow also provides a function you can use to get the size but this might not work. You can check if the size of the dataset is known using the following block of code.</p><pre id="46d7b5c0-83ac-42fa-8ace-6323706bc85b" class="code code-wrap"><code>cardinality = tf.data.experimental.cardinality(dataset)
print((cardinality == tf.data.experimental.UNKNOWN_CARDINALITY).numpy())</code></pre><p id="097f45fa-584c-4c24-a471-fb8a064b9a1d" class="">If this returns¬†<code>True</code>¬†then the size of the dataset is not known. If it returns¬†<code>False</code>¬†then it is known and you can get the size using¬†<code>tf.data.experimental.cardinality</code>¬†or¬†<code>dataset.cardinality</code>.</p><p id="a6292e38-e8ec-4c9b-bf69-bc016f8c882a" class="">Using the function below, you can create a train-val-test split.</p><pre id="f061d748-1f50-45c2-b16a-9046b1e62510" class="code code-wrap"><code>def train_val_test_split(dataset,
                         dataset_size,
                         train_split=0.8,
                         val_split=0.1,
                         test_split=0.1,
                         shuffle=True,
                         shuffle_size=10000):
    assert (train_split + test_split + val_split) == 1

    if shuffle:
      # Specify seed to always have the same split distribution between runs
        dataset = dataset.shuffle(shuffle_size, seed=12)

    train_size = int(train_split * dataset_size)
    val_size = int(val_split * dataset_size)

    train_dataset = dataset.take(train_size)
    val_dataset = dataset.skip(train_size).take(val_size)
    test_dataset = dataset.skip(train_size).skip(val_size)

    return train_dataset, val_dataset, test_dataset</code></pre><hr id="84f56bfc-f02b-4950-9f18-428dba939b0a"/><p id="24508988-9d91-427e-af01-bf2f79e68ffe" class="">by sodipeüåö on February 23, 2023.</p></div></article></body></html>